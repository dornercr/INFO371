{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO371/blob/main/INFO371_week6_7_Text_Representation_allMarkdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpAkZ28jzc9L"
      },
      "source": [
        "# INFO 371: Data Mining Applications\n",
        "\n",
        "## Week 6-7: Text Representation\n",
        "### Prof. Y. An, PhD\n",
        "### College of Computing and Informatics, Drexel University"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s69v8m28V9Fz"
      },
      "source": [
        "# Import Libraries\n",
        "- spaCy: spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.\n",
        "- pandas: Used for data manipulation and analysis\n",
        "- sklearn's CountVectorizer: Convert a collection of text documents to a matrix of token counts\n",
        "- sklearn's TfidfVectorizer: Convert a collection of raw documents to a matrix of TF-IDF features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByytxxwpzlOW"
      },
      "source": [
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmdoa81CO4QO"
      },
      "source": [
        "# Upload and read the text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03EQRtICcNy",
        "outputId": "24ff458b-dc88-471c-b236-00559bdb0cb0",
        "collapsed": true
      },
      "source": [
        "```\n",
        "files.upload()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWJH5h-5CXRj",
        "outputId": "9eaf5680-8b39-48c6-c901-ee101fe9ec90"
      },
      "source": [
        "```\n",
        "sms = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
        "sms.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "sms.shape\n",
        "```"
      ],
      "metadata": {
        "id": "XEimB-Nf4Ykh",
        "outputId": "d0ca4011-06e4-472d-91e6-24957d9cf97c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gelo785NDU13"
      },
      "source": [
        "```\n",
        "sms = sms[[\"v2\", \"v1\"]]\n",
        "sms.columns = [\"message\", \"label\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXzHtRPPCkTS",
        "outputId": "39ae1e73-53fd-4c17-e093-34c77417baae"
      },
      "source": [
        "```\n",
        "sms.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYAfVVyzDi1n",
        "outputId": "fd642e6b-1590-4e38-9184-db23ebc16708"
      },
      "source": [
        "```\n",
        "sms.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmAblk3DHrm-",
        "outputId": "f0c9633b-7a81-4b55-f738-34706457fd9b"
      },
      "source": [
        "```\n",
        "sms.loc[0].message\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Asyr395tPd",
        "outputId": "7e4f2991-17a8-4e39-bf4b-d7d0b2eccfe6"
      },
      "source": [
        "```\n",
        "sms.loc[2].message\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd3d6x8oSgFl"
      },
      "source": [
        "# Understanding the Data\n",
        "- It has five columns: v1, v2, and three unnamed columns.\n",
        "- The v1 column denotes the label of the text whether it is a spam or not.\n",
        "- The v2 column contains the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbZ-GSSSTHXU"
      },
      "source": [
        "# The label class distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEf4He3xTJ96",
        "outputId": "8fd604c6-990b-437a-c9f0-8dd42b00dfbe"
      },
      "source": [
        "```\n",
        "sms.label.value_counts()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUn1gS7qUdGX",
        "outputId": "3e604de3-7f81-4864-eaf5-04046d20a557"
      },
      "source": [
        "```\n",
        "sms.label.value_counts() / len(sms)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chGx63aLO_PT"
      },
      "source": [
        "# Spacy Tokenizer\n",
        "- We will use spaCy library for word tokenization\n",
        "- We will import spaCy English language model\n",
        "- We will remove stop words and punctuations\n",
        "- We will extract lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fn4Acuf5ypV"
      },
      "source": [
        "```\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POUJE--AVSOU"
      },
      "source": [
        "```\n",
        "doc = nlp(sms.loc[0].message)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "sms.loc[0].message\n",
        "```"
      ],
      "metadata": {
        "id": "Q0NgC56x5BFx",
        "outputId": "097f5639-83ba-492e-cc70-acf7df7c9132"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilj5OImGV4Fh",
        "outputId": "e989cd97-fb6f-4c79-b2bb-d6653a37e34d"
      },
      "source": [
        "```\n",
        "tokens_info = []\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JNC4peDPdvo",
        "outputId": "ca7987a5-5f24-449b-c1ec-6913fac28be0"
      },
      "source": [
        "```\n",
        "tokens_info = []\n",
        "for token in doc:\n",
        "    tokens_info.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_, \\\n",
        "            token.shape_, token.is_alpha, token.is_stop])\n",
        "tokens_df = pd.DataFrame(tokens_info, columns=['Token', 'Lemma', 'POS', 'TAG', 'DEP', 'Shape', 'Is_Alpha', 'Is_Stop'])\n",
        "tokens_df\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44W9DK33Qxyv"
      },
      "source": [
        "# Create a tokenizer using spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1K-ol6-PjOc",
        "outputId": "894d6d5b-4045-4cc6-a247-4432b72906c4"
      },
      "source": [
        "```\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Creating our tokenzer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    \"\"\"This function will accepts a sentence as input and processes the sentence into tokens, performing lemmatization,\n",
        "    lowercasing, removing stop words and punctuations.\"\"\"\n",
        "\n",
        "    # Creating our token object which is used to create documents with linguistic annotations\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # removing stop words and punctuations\n",
        "    mytokens = [word for word in doc if not word.is_stop and word.pos_ != 'PUNCT']\n",
        "\n",
        "    #lemmatizing each token and converting each token in lower case\n",
        "    mytokens = [word.lemma_.lower().strip() if word.pos_ != \"PRON\" else word.text.lower() for word in mytokens ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93N8ejnJYg_e",
        "outputId": "bac58cc9-ba22-451d-fc2f-206cb2023049"
      },
      "source": [
        "```\n",
        "spacy_tokenizer(sms.loc[345].message)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrievel practice on text pre-processing"
      ],
      "metadata": {
        "id": "dB-L9XFWldMq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-oYshUXcYvw"
      },
      "source": [
        "# Feature Engineering\n",
        "The objective is to predict whether a text is spam or not. For a classification model to understand the text,  we must convert them into numeric format.\n",
        "\n",
        "## Vectorization\n",
        "- We will convert labels to 1 or 0 such that spam=1 and ham=0\n",
        "- We are going to use Bag of Words(BoW) to convert text into numeric format.\n",
        "- BoW converts text into the matrix of occurrence of words within a given - document. It focuses on whether given word occurred or not in given document and generate the matrix called as BoW matrix/Document Term Matrix\n",
        "- We are going to use sklearn's CountVectorizer to generate BoW matrix.\n",
        "- In CountVectorizer we will use custom tokenizer 'spacy_tokenizer' and - ngram range to define the combination of adjacent words. So unigram means sequence of single word and bigrams means sequence of 2 continuous words.\n",
        "- Likewise, n means sequence of n continuous words.\n",
        "- In this example we are going to use unigram, so our lower and upper bound of ngram range will be (1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvZBRUfDaJ0-"
      },
      "source": [
        "```\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMnbZphTeG_3"
      },
      "source": [
        "## First, test binary vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxcxZi6mYnL4"
      },
      "source": [
        "```\n",
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range = (1,1), binary=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTwta3kUecL9",
        "outputId": "81360f34-dd51-492e-c87e-c7291d7bad20"
      },
      "source": [
        "```\n",
        "sms.loc[0].message\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64AK_oKdj1L",
        "outputId": "d1883690-f022-45fa-d44b-694b3186d1ca"
      },
      "source": [
        "```\n",
        "bow_vector.fit_transform(sms.loc[0:5].message).todense()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmchG0zLdxpX"
      },
      "source": [
        "```\n",
        "# Convert all text into vectors\n",
        "X = bow_vector.fit_transform(sms.message)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc0b10ktfa2R",
        "outputId": "5528f1b8-0550-4cc4-a3cb-cd141ecf07f6"
      },
      "source": [
        "```\n",
        "X.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjHiiS8-fqki",
        "outputId": "07766103-dc89-4d39-e1dc-948e18ba33fd"
      },
      "source": [
        "```\n",
        "# Convert class label to numeric 1 or 0\n",
        "y = sms.label.map({'spam':1, 'ham':0})\n",
        "y\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hiAn4jof_p9"
      },
      "source": [
        "# Split data into training and test sets\n",
        "- We will use sklearn train_test_split to create training and test sets\n",
        "- We will 80% of the data as training set and the rest 20% for test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJKghQJIf904"
      },
      "source": [
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1lVdTEtly6F"
      },
      "source": [
        "# Let us build a KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy24k3mcloOP"
      },
      "source": [
        "```\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cls = KNeighborsClassifier()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24V9bF6pmVXL"
      },
      "source": [
        "```\n",
        "scores = cross_val_score(cls, X_train, y_train, scoring='accuracy')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOytu5XBfer5",
        "outputId": "45a17134-a633-420c-b62c-d431752c7ba5"
      },
      "source": [
        "```\n",
        "scores\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnBSyntMmk7b",
        "outputId": "b66e36ed-e3e0-4ddf-f671-58ee09b68603"
      },
      "source": [
        "```\n",
        "np.mean(scores)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0pCXmHlmoys"
      },
      "source": [
        "# Test the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSErek6so8Yv",
        "outputId": "7334018c-b95f-4d96-d3c1-3e6b99eae2e4"
      },
      "source": [
        "```\n",
        "cls.fit(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuQkjwuNmnYx"
      },
      "source": [
        "```\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPKP_yOagJCP"
      },
      "source": [
        "```\n",
        "preds = cls.predict(X_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRFu8RBsgMFh",
        "outputId": "923bdc76-6765-4b81-dcdb-b0607f34977a"
      },
      "source": [
        "```\n",
        "preds.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlZIY71nght_",
        "outputId": "91522eaf-6162-4c49-c7b2-d438e3a56890"
      },
      "source": [
        "```\n",
        "accuracy_score(preds, y_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdNM6hsEgStR",
        "outputId": "f0de4172-be80-4a66-c126-7b1dee8a040c"
      },
      "source": [
        "```\n",
        "precision_score(preds, y_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niFL3gd6hhat",
        "outputId": "ca2bb148-476a-43a6-cf0b-a3aeb69336f4"
      },
      "source": [
        "```\n",
        "recall_score(preds, y_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA7KJ7eJf3c-",
        "outputId": "0cfc3332-c74d-41ba-a266-15fb5863d956"
      },
      "source": [
        "```\n",
        "print(\"Precision: {}\".format(precision_score(preds, y_test)))\n",
        "print(\"Recall: {}\".format(recall_score(preds, y_test)))\n",
        "print(\"F1-Measure: {}\".format(f1_score(preds, y_test)))\n",
        "print(\"Accuracy: {}\".format(accuracy_score(preds, y_test)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2qTZ4F2nJGU"
      },
      "source": [
        "```\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoFLrYEVp5QX"
      },
      "source": [
        "## Second, test count vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv3DnLBpp5Qg"
      },
      "source": [
        "```\n",
        "bow_vector_tf = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range = (1,1), binary=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlwZEnEgp5Qh",
        "outputId": "d43ba6ba-370d-4e16-b523-71801bee5fe2"
      },
      "source": [
        "```\n",
        "# Convert all text into vectors\n",
        "X = bow_vector_tf.fit_transform(sms.message)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaDFkquDp5Qh",
        "outputId": "c61db9e4-9af9-4e2f-bee0-478bcfffcf5a"
      },
      "source": [
        "```\n",
        "X.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNYrZVOjDg1",
        "outputId": "4ebb8208-1da0-4f51-f7c5-12105068f4d6"
      },
      "source": [
        "```\n",
        "X[0].todense()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsVfw9cp5Qh"
      },
      "source": [
        "# Split data into training and test sets\n",
        "- We will use sklearn train_test_split to create training and test sets\n",
        "- We will 80% of the data as training set and the rest 20% for test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nejdh4Ntp5Qi"
      },
      "source": [
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpTa_-J3p5Qi"
      },
      "source": [
        "# Let us build a KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsEO05vdp5Qi"
      },
      "source": [
        "```\n",
        "cls = KNeighborsClassifier()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvAMm0Jsp5Qj"
      },
      "source": [
        "```\n",
        "scores = cross_val_score(cls, X_train, y_train, scoring='accuracy')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FORP6CG7jVVw",
        "outputId": "5f994e7f-bd15-4ad1-d11c-d348ee6fe7d1"
      },
      "source": [
        "```\n",
        "scores\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnBV-v9Dp5Qj",
        "outputId": "4108425b-b3b5-4706-e341-4bdeb3f98cbc"
      },
      "source": [
        "```\n",
        "np.mean(scores)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3fYfxVip5Qj"
      },
      "source": [
        "# Test the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAfqo_yvp5Qj",
        "outputId": "a6d6dade-8161-4068-a4cb-f03927829771"
      },
      "source": [
        "```\n",
        "cls.fit(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3XOI7Z1p5Qk",
        "outputId": "33a49c79-8772-4e2c-a2f4-bbd7a49bc2d9"
      },
      "source": [
        "```\n",
        "preds = cls.predict(X_test)\n",
        "print(\"Precision: {}\".format(precision_score(preds, y_test)))\n",
        "print(\"Recall: {}\".format(recall_score(preds, y_test)))\n",
        "print(\"F1-Measure: {}\".format(f1_score(preds, y_test)))\n",
        "print(\"Accuracy: {}\".format(accuracy_score(preds, y_test)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval practice on binaryvector and countvector"
      ],
      "metadata": {
        "id": "xppr0Q3QozHR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw5dBaZGp5Qk"
      },
      "source": [
        "```\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnExbZgjp68S"
      },
      "source": [
        "## Test TFIDF vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmSlj4akp68T"
      },
      "source": [
        "```\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pvh3I8Rp68T",
        "outputId": "6ec7c8a8-31f3-432e-8a94-1233ba949b78"
      },
      "source": [
        "```\n",
        "# Convert all text into vectors\n",
        "X = tfidf_vector.fit_transform(sms.message)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16AKfDRTp68T",
        "outputId": "80262515-bd75-457e-da67-d93495bea95b"
      },
      "source": [
        "```\n",
        "X.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfS7JfLjkLL-",
        "outputId": "e4648f93-f6d2-479a-8e6d-d774edea6691"
      },
      "source": [
        "```\n",
        "(X[3678].toarray() != 0).sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kto_7koZp68U"
      },
      "source": [
        "# Split data into training and test sets\n",
        "- We will use sklearn train_test_split to create training and test sets\n",
        "- We will 80% of the data as training set and the rest 20% for test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1POsv2vVp68U"
      },
      "source": [
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxnEoJJCp68U"
      },
      "source": [
        "# Let us build a KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8AuClkIp68U"
      },
      "source": [
        "```\n",
        "cls = KNeighborsClassifier()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0nDxcsp68V"
      },
      "source": [
        "```\n",
        "scores = cross_val_score(cls, X_train, y_train, scoring='accuracy')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHb03An5kdf8",
        "outputId": "96e2fde9-3def-4590-dd0c-12f9630bd08b"
      },
      "source": [
        "```\n",
        "scores\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-ahtb6p68V",
        "outputId": "66dbd4dc-9a97-4c1f-dd15-006b01a1a1e7"
      },
      "source": [
        "```\n",
        "np.mean(scores)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIhW7N_Ep68V"
      },
      "source": [
        "# Test the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dqA3c8Mp68V",
        "outputId": "d1000243-c90d-4d13-b72e-36391abb0929"
      },
      "source": [
        "```\n",
        "cls.fit(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbjaj_-Ep68V",
        "outputId": "eda193fc-24bb-4a63-aaae-0057dfe10efd"
      },
      "source": [
        "```\n",
        "preds = cls.predict(X_test)\n",
        "print(\"Precision: {}\".format(precision_score(preds, y_test)))\n",
        "print(\"Recall: {}\".format(recall_score(preds, y_test)))\n",
        "print(\"F1-Measure: {}\".format(f1_score(preds, y_test)))\n",
        "print(\"Accuracy: {}\".format(accuracy_score(preds, y_test)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhpN_fVMp68V"
      },
      "source": [
        "```\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7moQvu_JwQ"
      },
      "source": [
        "## Test Word Embeddings\n",
        "- Use word2vec to embed each word in a message as a vector.\n",
        "- Use the mean of all word vectors in a message as the message embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# prompt: embed each message into an embedding vector using word2vec\n",
        "\n",
        "import gensim.downloader as gensim\n",
        "```"
      ],
      "metadata": {
        "id": "d5pJqrNgHJ6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Load a pre-trained Word2Vec model (e.g., 'word2vec-google-news-300')\n",
        "\n",
        "#word2vec = gensim.load('word2vec-google-news-300')\n",
        "```"
      ],
      "metadata": {
        "id": "geYxQlAvHYFb",
        "outputId": "eff51542-0023-430e-9af4-bc23474ddc4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector for a given text using Word2Vec.\"\"\"\n",
        "    tokens = spacy_tokenizer(text)  # Assuming spacy_tokenizer is defined in the previous code\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            vectors.append(word2vec[token])\n",
        "        except KeyError:\n",
        "            # Handle out-of-vocabulary words (e.g., use a zero vector)\n",
        "            vectors.append(np.zeros(300))  # Assuming the embedding dimension is 300\n",
        "\n",
        "    if vectors:  # Check if there are any valid word embeddings\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(300)  # Return a zero vector if no valid word embeddings are found\n",
        "```"
      ],
      "metadata": {
        "id": "1O4YzbozHqBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from tqdm import tqdm\n",
        "```"
      ],
      "metadata": {
        "id": "JAcRwPZDLytu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Embed each message as a vector\n",
        "message_embeddings = []\n",
        "for message in tqdm(sms['message']):\n",
        "    message_embeddings.append(get_embedding(message))\n",
        "```"
      ],
      "metadata": {
        "id": "oXNJJPiI_P8M",
        "outputId": "8d47a73e-8261-4468-9909-a8bb3c7c27e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "X = np.array(message_embeddings)\n",
        "X.shape\n",
        "```"
      ],
      "metadata": {
        "id": "64QkCueLIFLj",
        "outputId": "8d8bb4a1-9c12-45fd-cf2f-25b5a9e289f6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2-E6xSs_JwR"
      },
      "source": [
        "# Split data into training and test sets\n",
        "- We will use sklearn train_test_split to create training and test sets\n",
        "- We will 80% of the data as training set and the rest 20% for test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn1f_Cr5_JwR"
      },
      "source": [
        "```\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWETIXU6_JwR"
      },
      "source": [
        "# Let us build a KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3wc5S1_JwS"
      },
      "source": [
        "```\n",
        "cls = KNeighborsClassifier()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC_kjuF7_JwS"
      },
      "source": [
        "```\n",
        "scores = cross_val_score(cls, X_train, y_train, scoring='accuracy')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "0a5e63c4-3f01-4b78-e2e4-8cb14238c519",
        "id": "rUNJQU4C_JwS"
      },
      "source": [
        "```\n",
        "scores\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "2dd87983-9830-4dee-b263-95ac6b5af34e",
        "id": "XHpAk09H_JwS"
      },
      "source": [
        "```\n",
        "np.mean(scores)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RV-IarC_JwS"
      },
      "source": [
        "# Test the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "ce750e16-5c46-4ab4-9d9a-333c6c5e5fb6",
        "id": "Nkn2o8j3_JwS"
      },
      "source": [
        "```\n",
        "cls.fit(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "5df3fd0d-090a-46a8-caba-1788a6059aaa",
        "id": "OFYcG8sh_JwS"
      },
      "source": [
        "```\n",
        "preds = cls.predict(X_test)\n",
        "print(\"Precision: {}\".format(precision_score(preds, y_test)))\n",
        "print(\"Recall: {}\".format(recall_score(preds, y_test)))\n",
        "print(\"F1-Measure: {}\".format(f1_score(preds, y_test)))\n",
        "print(\"Accuracy: {}\".format(accuracy_score(preds, y_test)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval practice on tfidf and embeddings"
      ],
      "metadata": {
        "id": "3lBdrPRlqr7W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23asKaXR_JwS"
      },
      "source": [
        "```\n",
        "\n",
        "```"
      ]
    }
  ]
}