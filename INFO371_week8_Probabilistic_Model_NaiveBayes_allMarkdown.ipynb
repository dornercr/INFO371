{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/INFO371/blob/main/INFO371_week8_Probabilistic_Model_NaiveBayes_allMarkdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpAkZ28jzc9L"
      },
      "source": [
        "# INFO 371: Data Mining Applications\n",
        "\n",
        "## Week 8: Probabilitic Model and Naive Bayes\n",
        "### Prof. Charles Dorner, EdD (Candidate)\n",
        "### College of Computing and Informatics, Drexel University"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A probabilistic classifier:\n",
        "- Given an observation of an input\n",
        "- Predict a probability distribution over a set of classes\n",
        "- rather than only outputting the most likely class that the observation should belong to.\n",
        "\n",
        "## For example,\n",
        "- Given an Email as a text document:\n",
        " - $Pr(Spam|Email)$ = 0.7\n",
        " - $Pr(Not\\_Spam|Email)$| = 0.3\n"
      ],
      "metadata": {
        "id": "xvgOwOSMto9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayes' Theorem in the Context of Data Mining\n",
        "\n",
        "\\begin{equation}\n",
        "P(H \\mid D) = \\frac{P(D \\mid H) P(H)}{P(D)}\n",
        "\\end{equation}\n",
        "\n",
        "- $P(H‚à£D)$: Posterior Probability: The probability of hypothesis\n",
        "$ùêª$ (e.g., a model or pattern being true) given the observed data $ùê∑$.\n",
        "\n",
        "- $P(D‚à£H)$: Likelihood: The probability of the data occurring given that hypothesis $ùêª$. In data mining, this represents how well the data supports a specific model.\n",
        "\n",
        "- $P(H)$: Prior Probability: The initial belief about hypothesis\n",
        "$ùêª$ before observing the data. In data mining, this may come from domain knowledge or historical patterns.\n",
        "\n",
        "- $P(D)$: Evidence (Marginal Probability of Data): The overall probability of observing the data, regardless of which hypothesis is true. This acts as a normalizing factor.\n"
      ],
      "metadata": {
        "id": "UmAajeF7u150"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability Rules for Two Events \\( A \\) and \\( B \\)\n",
        "\n",
        "## 1. Mutually Exclusive Events\n",
        "Two events \\( A \\) and \\( B \\) are **mutually exclusive** if they cannot occur together. That is:\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = 0\n",
        "$$\n",
        "\n",
        "Using the addition rule:\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B)\n",
        "$$\n",
        "\n",
        "## 2. Not Mutually Exclusive Events\n",
        "If \\( A \\) and \\( B \\) are **not mutually exclusive**, they can occur together. The general addition rule applies:\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
        "$$\n",
        "\n",
        "## 3. Independent Events\n",
        "Two events \\( A \\) and \\( B \\) are **independent** if the occurrence of one does not affect the probability of the other. This means:\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = P(A) P(B)\n",
        "$$\n",
        "\n",
        "## 4. Dependent Events\n",
        "If \\( A \\) and \\( B \\) are **dependent**, the probability of one event depends on the occurrence of the other. The conditional probability rule applies:\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = P(A \\mid B) P(B) = P(B \\mid A) P(A)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "pIxEHfWbxV9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Example\n",
        "- Sentimental analysis: classify whether a review is positive or negative\n",
        " - ‚ÄúThe author is making big money‚Äù (positive)\n",
        " - ‚ÄúIrony but fascinating‚Äù (positive)\n",
        " - ‚Äúdon‚Äôt waste money on it‚Äù (negative)\n",
        "\n",
        "- Is ‚Äúmoney wasted, fascinated‚Äù positive or negative?"
      ],
      "metadata": {
        "id": "LYIijUbrw_80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "s = {\"The author is making big money\":1, \"Irony but fascinating\":1,\n",
        "\"don‚Äôt waste money on it\":0}\n",
        "s\n",
        "```"
      ],
      "metadata": {
        "id": "7RJtLegAxchu",
        "outputId": "ef1b0c9f-81ef-4c3a-e077-8b23f0fbcf03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the libraries for tokenization and normalization"
      ],
      "metadata": {
        "id": "BhObUfvQYgch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load spaCy's English tokenizer, tagger, parser, and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "```"
      ],
      "metadata": {
        "id": "qMrhJc7WyquV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize, normalize, and count word frequencies"
      ],
      "metadata": {
        "id": "svhl0Tt31GnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "word_freq_pos = Counter()\n",
        "word_freq_neg = Counter()\n",
        "\n",
        "for sentence in s:\n",
        "    doc = nlp(sentence.lower())  # Convert to lowercase for uniformity\n",
        "    if s[sentence] == 1:\n",
        "        words = [stemmer.stem(token.lemma_.lower()) for token in doc if token.is_alpha]  # Keep only words\n",
        "        word_freq_pos.update(words)\n",
        "    else:\n",
        "        words = [stemmer.stem(token.lemma_ .lower()) for token in doc if token.is_alpha]  # Keep only words\n",
        "        word_freq_neg.update(words)\n",
        "```"
      ],
      "metadata": {
        "id": "2ShfZPTCyv0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "word_freq_pos, word_freq_neg\n",
        "```"
      ],
      "metadata": {
        "id": "ezTVH5cizsWM",
        "outputId": "fbca37c7-786f-4d16-aebe-8e084e165aef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Word Probabiliies\n",
        "- Compute the conditional probabily of each word given a class\n",
        "- Given a word $w$,\n",
        " \\begin{equation}\n",
        " Pr(w| pos) = \\frac{frequency\\ of\\ w\\ in\\ all\\ positive\\ cases}{total\\ number\\ of\\ positive\\ cases}\n",
        " \\end{equation}\n",
        "\n",
        " \\begin{equation}\n",
        " Pr(w| neg) = \\frac{frequency\\ of\\ w\\ in\\ all\\ negative\\ cases}{total\\ number\\ of\\ negative\\ cases}\n",
        " \\end{equation}."
      ],
      "metadata": {
        "id": "b7sTrn6G1Ji2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "word_prob_pos = {}\n",
        "word_prob_neg = {}\n",
        "```"
      ],
      "metadata": {
        "id": "H-7GeOag1SAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "for word, count in word_freq_pos.items():\n",
        "    word_prob_pos[word] = count / sum(word_freq_pos.values())\n",
        "for word, count in word_freq_neg.items():\n",
        "    word_prob_neg[word] = count / sum(word_freq_neg.values())\n",
        "```"
      ],
      "metadata": {
        "id": "Q0cqls0v1W04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "word_prob_pos, word_prob_neg\n",
        "```"
      ],
      "metadata": {
        "id": "fR71tVa01egi",
        "outputId": "ebcdc496-5393-40de-adbc-368fafaa07a1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Classification\n",
        "- Tokenize and normalize the given sentence instance $s$.\n",
        "- Compute the conditional probabilies, $Pr(pos|s)$ and $Pr(neg|s)$.\n",
        "- Apply Bayes' Theorem:\n",
        "\\begin{equation}\n",
        "    Pr(pos|s) = \\frac{Pr(s|pos)\\times Pr(pos)}{Pr(s)}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    Pr(pos|s) = \\frac{Pr(s|pos)\\times Pr(pos)}{Pr(s)}\n",
        "\\end{equation}\n",
        "- Apply word independency assumption:\n",
        "\\begin{equation}\n",
        "    Pr(s|pos) = Pr(w1|pos)\\times Pr(w2|pos)\\times...\\times Pr(w_{m}|pos)\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    Pr(s|neg) = Pr(w1|neg)\\times Pr(w2|neg)\\times...\\times Pr(w_{m}|neg)\n",
        "\\end{equation}\n",
        "\n",
        "- Make comparison:\n",
        "\\begin{equation}\n",
        "    Pr(pos|s) \\sim Pr(s|pos)\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    Pr(neg|s) \\sim Pr(s|neg)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "a92_izzR1jWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "ss = \"money wasted, fascinated\"\n",
        "doc = nlp(ss.lower())\n",
        "words = [stemmer.stem(token.lemma_.lower()) for token in doc if token.is_alpha]\n",
        "words\n",
        "```"
      ],
      "metadata": {
        "id": "lZhJjotX1nYU",
        "outputId": "e105d24a-7d39-4781-fcbe-2f569045e0d3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Pr_1 = 1\n",
        "for word in words:\n",
        "    if word in word_prob_pos:\n",
        "        Pr_1 *= word_prob_pos[word]\n",
        "Pr_1\n",
        "```"
      ],
      "metadata": {
        "id": "qCY7q_HxFu29",
        "outputId": "61005814-6eaa-4648-be37-da0ceee5efda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Pr_0 = 1\n",
        "for word in words:\n",
        "    if word in word_prob_neg:\n",
        "        Pr_0 *= word_prob_neg[word]\n",
        "Pr_0\n",
        "```"
      ],
      "metadata": {
        "id": "hB6XelsnGHox",
        "outputId": "b4f531fb-f49e-4601-e5f1-07c871e7f04d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# normalize the probabilities\n",
        "Pr_1_norm = Pr_1 / (Pr_1 + Pr_0)\n",
        "Pr_0_norm = Pr_0 / (Pr_1 + Pr_0)\n",
        "Pr_1_norm, Pr_0_norm\n",
        "```"
      ],
      "metadata": {
        "id": "9gXfDzuIGJoE",
        "outputId": "c52d37fd-eb60-432e-eea1-5206c3bf9540"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "gzux_RJSgsFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Scikit Learn Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "xKm8BCxqg1R2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByytxxwpzlOW"
      },
      "source": [
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmdoa81CO4QO"
      },
      "source": [
        "## Upload and read the text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03EQRtICcNy",
        "outputId": "b13bfa6c-9f1b-42ba-cbcf-63641e02e36f",
        "collapsed": true
      },
      "source": [
        "```\n",
        "files.upload()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWJH5h-5CXRj",
        "outputId": "10ffc905-0ab6-42ec-b94f-80667e3514ab"
      },
      "source": [
        "```\n",
        "sms = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
        "sms.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXzHtRPPCkTS",
        "outputId": "b04aad4f-6908-4ff2-ec56-31df34527878"
      },
      "source": [
        "```\n",
        "sms.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Distribution"
      ],
      "metadata": {
        "id": "xXuZGQ6qhMXs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEf4He3xTJ96",
        "outputId": "4731dd39-a31d-4ff4-f626-c1a6ec5efc9c"
      },
      "source": [
        "```\n",
        "sms.v1.value_counts()/sms.shape[0]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44W9DK33Qxyv"
      },
      "source": [
        "## Create a tokenizer using spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1K-ol6-PjOc"
      },
      "source": [
        "```\n",
        "# Creating our tokenzer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    \"\"\"This function will accepts a sentence as input and processes the sentence into tokens, performing lemmatization,\n",
        "    lowercasing, removing stop words and punctuations.\"\"\"\n",
        "\n",
        "    # Creating our token object which is used to create documents with linguistic annotations\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # removing stop words and punctuations\n",
        "    mytokens = [word for word in doc if not word.is_stop and word.pos_ != 'PUNCT']\n",
        "\n",
        "    #lemmatizing each token and converting each token in lower case\n",
        "    mytokens = [word.lemma_.lower().strip() if word.pos_ != \"PRON\" else word.text.lower() for word in mytokens ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93N8ejnJYg_e",
        "outputId": "02b56dc8-319f-4a42-f05c-dbaf33962ac5"
      },
      "source": [
        "```\n",
        "spacy_tokenizer(sms.loc[4].v2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-oYshUXcYvw"
      },
      "source": [
        "## Vectorization\n",
        "- We will convert labels to 1 or 0 such that spam=1 and ham=0\n",
        "- We are going to use Bag of Words(BoW) to convert text into numeric format.\n",
        "- BoW converts text into the matrix of occurrence of words within a given - document. It focuses on whether given word occurred or not in given document and generate the matrix called as BoW matrix/Document Term Matrix\n",
        "- We are going to use sklearn's CountVectorizer to generate BoW matrix.\n",
        "- In CountVectorizer we will use custom tokenizer 'spacy_tokenizer' and - ngram range to define the combination of adjacent words. So unigram means sequence of single word and bigrams means sequence of 2 continuous words.\n",
        "- Likewise, n means sequence of n continuous words.\n",
        "- In this example we are going to use unigram, so our lower and upper bound of ngram range will be (1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZUbhmBcdbk3"
      },
      "source": [
        "```\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxcxZi6mYnL4"
      },
      "source": [
        "```\n",
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range = (1,1))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmchG0zLdxpX",
        "outputId": "c2404b91-89a5-4f8a-98fe-62f9ced94f37"
      },
      "source": [
        "```\n",
        "# Convert all text into vectors\n",
        "X = bow_vector.fit_transform(sms.v2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc0b10ktfa2R",
        "outputId": "d80e5d5e-6934-4dc2-8958-c55b40343b78"
      },
      "source": [
        "```\n",
        "X.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "X.todense()[:2]\n",
        "```"
      ],
      "metadata": {
        "id": "rtkKqYREiBUD",
        "outputId": "b7080c2d-a5bd-448e-d07f-27fa11004312"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "bow_vector.vocabulary_\n",
        "```"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oyZTfNZmihQp",
        "outputId": "196f09c2-a9a6-4a42-fb9b-af1f91c9ccbb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjHiiS8-fqki",
        "outputId": "692fbf12-b34e-4605-d009-7f0cb6e003ef"
      },
      "source": [
        "```\n",
        "# Convert class label to numeric 1 or 0\n",
        "y = sms.v1.map({'spam':1, 'ham':0})\n",
        "y\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hiAn4jof_p9"
      },
      "source": [
        "## Split data into training and test sets\n",
        "- We will use sklearn train_test_split to create training and test sets\n",
        "- We will 80% of the data as training set and the rest 20% for test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJKghQJIf904"
      },
      "source": [
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1lVdTEtly6F"
      },
      "source": [
        "## Let us build a Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy24k3mcloOP"
      },
      "source": [
        "```\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cls = MultinomialNB()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24V9bF6pmVXL",
        "outputId": "578069c3-8716-4613-a002-896ea9e72ac8"
      },
      "source": [
        "```\n",
        "scores = cross_val_score(cls, X_train, y_train, scoring='accuracy')\n",
        "scores\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnBSyntMmk7b",
        "outputId": "af2264af-7a75-40c7-9397-a775a587398e"
      },
      "source": [
        "```\n",
        "np.mean(scores)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0pCXmHlmoys"
      },
      "source": [
        "## Test the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSErek6so8Yv",
        "outputId": "46eb21cd-d2b4-453b-bcf1-1d45e1e34e57"
      },
      "source": [
        "```\n",
        "cls.fit(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuQkjwuNmnYx",
        "outputId": "88c341c5-9c07-4ca9-9d3c-3da8913c46ae"
      },
      "source": [
        "```\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "preds = cls.predict(X_test)\n",
        "print(\"Precision: {}\".format(precision_score(preds, y_test)))\n",
        "print(\"Recall: {}\".format(recall_score(preds, y_test)))\n",
        "print(\"F1-Measure: {}\".format(f1_score(preds, y_test)))\n",
        "print(\"Accuracy: {}\".format(accuracy_score(preds, y_test)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "9OA0HotijfmV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2qTZ4F2nJGU"
      },
      "source": [
        "```\n",
        "\n",
        "```"
      ]
    }
  ]
}