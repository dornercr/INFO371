{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/INFO371/blob/main/INFO371_week8_NN_image_classification_allMarkdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADxFD_qNz_6L"
      },
      "source": [
        "# INFO 371: Data Mining Applications\n",
        "\n",
        "## Week 8: Neural Networks for Image Classification with Keras, Tensorflow, and PyTorch\n",
        "### Charles Dorner, EdD (Candidate)\n",
        "### College of Computing and Informatics, Drexel University"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "```"
      ],
      "metadata": {
        "id": "CohBXWZAFr7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification\n",
        "- Image classification is a fundamental task in computer vision where an algorithm assigns labels to images based on their content.\n",
        "- Automatically categorize images into predefined classes (e.g., \"cat,\" \"dog,\" \"car\")."
      ],
      "metadata": {
        "id": "d4m_N_Q4EVUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Data as a Multi-Dimensional Array\n",
        "Images are typically represented as multi-dimensional arrays (tensors). The structure of these arrays depends on the image type and color format.\n",
        "\n",
        "- Grayscale Image (2D Array): A grayscale image has only one channel (intensity values).\n",
        "- Shape: (height, width)"
      ],
      "metadata": {
        "id": "axH6LhhjFV2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# load the keras CIFAR dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories.\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "```"
      ],
      "metadata": {
        "id": "ZDcT2_sjH5jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Retriev an image\n",
        "aimage = X_train[100]\n",
        "aimage.shape\n",
        "```"
      ],
      "metadata": {
        "id": "exF29Zr6Fk8A",
        "outputId": "78e3397c-744c-4742-85a8-d4b2031ac863"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# View the original image 32x32x3\n",
        "plt.figure(figsize=(1,1))\n",
        "plt.imshow(aimage)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "T4hy_7MaLT4P",
        "outputId": "4be43243-cae3-4985-b274-90109da334b4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Retrieve only one channel 32x32\n",
        "gray_image = aimage[:,:, 0]\n",
        "gray_image.shape\n",
        "```"
      ],
      "metadata": {
        "id": "8476L0I8MsuS",
        "outputId": "ccfe0e26-bc3a-47ce-8d16-83ff268c2aa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "gray_image\n",
        "```"
      ],
      "metadata": {
        "id": "5lWxtKjdM_oM",
        "outputId": "a8c1ed47-a534-4fe6-9e3c-f48dc80f9098"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# plot the one channel image\n",
        "plt.imshow(gray_image, cmap='gray')\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "AICYHshEI6N1",
        "outputId": "c5ddc946-8026-4313-be83-8f7b4d7f61aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RGB Image (3D Array): A color image has three channels: Red, Green, and Blue (RGB).\n",
        "- Shape: (height, width, 3)"
      ],
      "metadata": {
        "id": "kFMWk_DUGOOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# show the 32x32x3 data\n",
        "aimage\n",
        "```"
      ],
      "metadata": {
        "id": "_GiGf8_zGa5h",
        "outputId": "86f832ee-df2c-40b3-fea7-7ae2ea03a12d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "plt.imshow(aimage)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "y83Xdj_6Njch",
        "outputId": "57f2ed16-d32e-42a6-8d61-dc44dfa91955"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Batch of Images (4D Tensor): When processing multiple images in deep learning, they are stored in batch form.\n",
        "- Shape: (batch_size, height, width, channels)\n"
      ],
      "metadata": {
        "id": "G8t7anuSGpWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "X_train.shape\n",
        "```"
      ],
      "metadata": {
        "id": "_XMPiJUhGwW5",
        "outputId": "cbeb8d5e-8e7e-43c7-9ee7-8b4db16384a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification by Logistic Regression\n",
        "- Let us consider a binary classification problem.\n",
        "- To classify an image into (yes, no) labels.\n",
        "- A (height, width, channels) image is flatten to a 1-D vector.\n",
        "- A linear combination of the pixels followed by the logistic function gives rise to the classification.\n",
        "\n",
        "![](https://i.imgur.com/3dfiaKI.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_FVMI-YkN2-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Artificial Neural Network\n",
        "\n",
        "- An Artificial Neural Network (ANN) is a computational model inspired by the structure and function of the human brain.\n",
        "- It consists of layers of interconnected artificial neurons that process and learn from data to recognize patterns, make predictions, and solve complex problems.\n",
        "- Each neuron is just an extension of logistic regression.\n",
        "\n",
        "![](https://i.imgur.com/7yrbeiU.png)\n",
        "\n",
        "## Key Components of ANN:\n",
        "- Neurons (Nodes): Basic units that receive input, process it, and pass output.\n",
        "- Layers:\n",
        " - Input Layer: Takes raw data as input.\n",
        " - Hidden Layers: Perform computations and extract patterns.\n",
        " - Output Layer: Produces the final result or prediction.\n",
        " - Weights & Biases: Parameters that adjust during training to improve accuracy.\n",
        " - Activation Functions: Determine the output of a neuron (e.g., Sigmoid, ReLU, Tanh).\n",
        " - Learning Process: Uses algorithms like Backpropagation and Gradient Descent to update weights based on errors.\n",
        "\n",
        "\n",
        " ![](https://i.imgur.com/MZ75NgI.png)"
      ],
      "metadata": {
        "id": "FvZqjxNePLvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Single Neuron and Logistic Regression\n",
        "\n",
        "- A single neuron is just an extension of logistic regression, which itself is just an extension of linear regression.\n",
        "- If the inputs are represented by the variables $x_1, x_2, ..., x_i$, and the weights controlling the influence of these inputs is represented by $w_1, w_2, ..., w_i$ then the equation for linear regression is:\n",
        "\n",
        "$$y=w_1x_1 + w_2x_2 + ... + w_ix_i$$\n",
        "\n",
        "- For logistic regression, we can modify this one step further, by passing the entire output through the logistic function $f(z)=\\frac{1}{1+e^{-z}}$.\n",
        "- As a result, the output is between 0 and 1 as computed by the following equation:\n",
        "\n",
        "$$y=f(w_1x_1 + w_2x_2 + ... + w_ix_i)$$\n",
        "\n",
        "- For example, the following neuron shows:\n",
        " * three inputs ($x_1, x_2, x_3$)\n",
        " * three weights ($w_1, w_2, w_3$)\n",
        " * an output: $y=f(w_1x_1 + w_2x_2 + w_3x_3)$\n",
        "\n",
        " ![logistic regression](https://i.imgur.com/uCB4OcQ.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "z0uMYUndXpQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Function\n",
        "- Activation functions are simply a function that is applied to the weighed sum of a neuron.\n",
        "- A list of some common activation functions and their graphs can be seen below.\n",
        "\n",
        " - Relu (Rectified Linear Unit):\n",
        "\n",
        "  ![](https://yashuseth.files.wordpress.com/2018/02/relu-function.png?w=309&h=274)\n",
        " - Tanh (Hyperbolic Tangent):\n",
        "\n",
        " ![](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
        " - Sigmoid:\n",
        "\n",
        " ![](https://miro.medium.com/max/970/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "cTY-NUyXfkjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "0SNvYPgK2nR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss/Cost Function\n",
        "- A loss/cost function is responsible for determining how well the network preformed.\n",
        "- Some common loss/cost functions include.\n",
        " - Mean Squared Error:\n",
        " $$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
        "\n",
        " - Mean Absolute Error:\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "where $ y_i $ is the actual value, $ \\hat{y}_i $ are as above.\n",
        " - Hinge Loss:\n",
        "$$\n",
        "\\text{Hinge Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - y_i\\cdot \\hat{y}_i)\n",
        "$$\n",
        "where $ y_i $ is the true label $(\\pm 1$) and $ \\hat{y}_i $ is the predicted score.\n",
        " - Binary Cross-Entropy (Log Loss)}:\n",
        " For binary classification, where $ y \\in \\{0,1\\} $, the loss function is given by:\n",
        "$$\n",
        "\\text{Log Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]\n",
        "$$\n",
        "where: $ y_i $ is the true label (0 or 1), $ \\hat{y}_i $ is the predicted probability for class 1, $ n $ is the number of samples.\n",
        "\n",
        "  - Categorical Cross-Entropy: For multi-class classification with $ C $ classes, the cross-entropy loss is:\n",
        "$$\n",
        "\\text{Categorical Log Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} y_{i,j} \\log \\hat{y}_{i,j}\n",
        "$$\n",
        "where: $ y_{i,j} $ is 1 if the true class of sample $ i $ is $ j $, otherwise 0, $\\hat{y}_{i,j}$ is the predicted probability for class $ j $, $ C $ is the total number of classes."
      ],
      "metadata": {
        "id": "ijHI4R3xglCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Neural Network\n",
        "- Typically, gradient descent is applied to learning the weights and biases of a neural network:\n",
        " 1. Randomly initialize the weights of the neurons to small values.\n",
        " 2. Choose a loss function (some measure of how poorly our model is predicting the training data)\n",
        " 3. Calculate the model's loss on the training data\n",
        " 4. Update the weights in the direction that reduces loss (using calculus)\n",
        " 5. Repeat steps 3 and 4 until we're happy with the network's performance\n",
        "\n"
      ],
      "metadata": {
        "id": "kZ2MQ11yaRLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent for Log Loss $L$\n",
        "\n",
        "Let the predicted probability be:\n",
        "$$\n",
        "\\hat{y}_i = \\sigma (w^T x_i) = \\frac{1}{1 + e^{-w^T x_i}}\n",
        "$$\n",
        "where $ \\sigma(z) $ is the sigmoid function.\n",
        "\n",
        "The gradient of the loss function with respect to the weights $ w $ is:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) x_i\n",
        "$$\n",
        "\n",
        "## Gradient Descent Update Rule\n",
        "Using the gradient descent algorithm, the weights are updated as:\n",
        "$$\n",
        "w := w - \\alpha \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "where $ \\alpha $ is the learning rate.\n",
        "\n",
        "Expanding the update rule:\n",
        "$$\n",
        "w := w - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) x_i\n",
        "$$\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*iU1QCnSTKrDjIPjSAENLuQ.png)\n"
      ],
      "metadata": {
        "id": "U9vzwyjs98y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "- An algorithm which is used to train artificial neural networks by adjusting the weights to minimize the error.\n",
        "- It works by propagating the error backward from the output layer to the input layer, updating weights using gradient descent."
      ],
      "metadata": {
        "id": "cUD-_TmZ5W-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "L66-rFkx2rwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Keras for Classification"
      ],
      "metadata": {
        "id": "MdVH2XSIINzx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhFSpp2Iz_Pm"
      },
      "source": [
        "```\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        " # Load the data and split it between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "```"
      ],
      "metadata": {
        "id": "tpqxa9fnIeLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "X_train.shape\n",
        "```"
      ],
      "metadata": {
        "id": "N374Suv_Io-F",
        "outputId": "7946e3d8-3ab5-4f26-f0ed-1c50980cdbaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "X_train[0].min(), X_train[0].max()\n",
        "```"
      ],
      "metadata": {
        "id": "M9qZ1sSqLJXC",
        "outputId": "b24364eb-0628-46b5-83ed-de98aeebc122"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Scale images to the [0, 1] range\n",
        "X_train = X_train.astype(\"float32\") / 255\n",
        "X_test = X_test.astype(\"float32\") / 255\n",
        "```"
      ],
      "metadata": {
        "id": "3MGZDKNXLlZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "X_train[0].min(), X_train[0].max()\n",
        "```"
      ],
      "metadata": {
        "id": "XJELo3QcLyzk",
        "outputId": "dcb77557-b2b8-485d-d5a9-c1fba4e5af85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Plot random 4x4 MNIST images\n",
        "grid_size=(4, 4)\n",
        "fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(8, 8))\n",
        "indices = np.random.choice(X_train.shape[0], grid_size[0] * grid_size[1], replace=False)\n",
        "\n",
        "for ax, idx in zip(axes.flat, indices):\n",
        "    ax.imshow(X_train[idx], cmap='gray')\n",
        "    ax.set_xticks([])  # Remove x ticks\n",
        "    ax.set_yticks([])  # Remove y ticks\n",
        "    ax.set_xlabel(f\"Label: {y_train[idx]}\", fontsize=10)  # Show label below image\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "jfl1HR0dJ8TI",
        "outputId": "5ff16870-c7d2-49df-fbf7-6414258b7494"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(X_train.shape[0], \"train samples\")\n",
        "print(X_test.shape[0], \"test samples\")\n",
        "```"
      ],
      "metadata": {
        "id": "_5tasJnMKfEn",
        "outputId": "4810c6d0-40ba-4f16-ac27-ced3b4056e24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Model parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=input_shape),\n",
        "    keras.layers.Flatten(),  # Flatten the 28x28 image into a 1D vector\n",
        "    keras.layers.Dense(512, activation=\"relu\"),\n",
        "    keras.layers.Dense(256, activation=\"relu\"),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),  # Regularization to prevent overfitting\n",
        "    keras.layers.Dense(num_classes, activation=\"softmax\")  # Output layer for classification\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "```"
      ],
      "metadata": {
        "id": "NoF7OFMlNUzZ",
        "outputId": "fa86aedf-a428-49c8-a38d-423241271b73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Compile the NN\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "    ],\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "g90FLogMNmn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.15\n",
        ")\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "```"
      ],
      "metadata": {
        "id": "TjAhm9MsNyEb",
        "outputId": "6e7e010e-a375-4d89-811a-da1366d2b982"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "score\n",
        "```"
      ],
      "metadata": {
        "id": "aUsBVV1SORwX",
        "outputId": "4edeace2-b010-4aa9-f950-c03a10d5075a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "predictions = model.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "hGi2JveDOrM-",
        "outputId": "4e29985d-d023-453d-a1f5-f9237fac5727"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "accuracy_score(y_test, predictions.argmax(axis=1))\n",
        "```"
      ],
      "metadata": {
        "id": "yclKAhgTOtm3",
        "outputId": "7ff2194d-049a-49e5-8136-11c5d8064d9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "precision_recall_fscore_support(y_test, predictions.argmax(axis=1))\n",
        "```"
      ],
      "metadata": {
        "id": "CJQhNdJSO9_p",
        "outputId": "c8764e04-a4ce-4017-d060-7985a9468b30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "zcA9mYUn2wab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convoluational Neural Network\n",
        "- Uses convolutional layers to extract spatial features, followed by pooling layers to reduce dimensions.\n",
        "- Designed for 2D/3D structured data like images, videos, or volumetric data.\n",
        "- Automatically extracts spatial and hierarchical features using filters/kernels.\n",
        "- Preserves local spatial structures using convolution operations."
      ],
      "metadata": {
        "id": "GQOkPvMDPR-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "import matplotlib.animation as animation\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "# Create a 6×6 input image (example)\n",
        "image = np.random.rand(6, 6)\n",
        "\n",
        "#image = np.array([\n",
        "#    [1, 1, 1, 1, 1, 0],\n",
        "#    [0, 0, 0, 0, 1, 0],\n",
        "#    [0, 0, 1, 1, 1, 0],\n",
        "#    [0, 0, 0, 0, 1, 0],\n",
        "#    [0, 0, 0, 0, 1, 0],\n",
        "#    [1, 1, 1, 1, 1, 0],\n",
        "#])\n",
        "\n",
        "# Define a 3×3 filter (edge detection example)\n",
        "kernel = np.array([\n",
        "    [1,  0, -1],\n",
        "    [1,  0, -1],\n",
        "    [1,  0, -1]\n",
        "])\n",
        "\n",
        "# Perform 2D convolution\n",
        "feature_map = convolve2d(image, kernel, mode=\"valid\")\n",
        "\n",
        "feature_map\n",
        "```"
      ],
      "metadata": {
        "id": "S-WY3AOarZuY",
        "outputId": "e3c999e0-667a-4f49-d0a7-fc3dccc0943f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Animation setup\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "# Initialize images\n",
        "ax1, ax2 = axes\n",
        "img1 = ax1.imshow(image, cmap=\"gray\", interpolation=\"none\")\n",
        "img2 = ax2.imshow(feature_map, cmap=\"gray\", interpolation=\"none\")\n",
        "\n",
        "ax1.set_title(\"Original Image\")\n",
        "ax2.set_title(\"Feature Map\")\n",
        "\n",
        "# Disable axis ticks\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "\n",
        "# Highlight the sliding filter (kernel) position\n",
        "kernel_size = 2\n",
        "rect = plt.Rectangle((0, 0), kernel_size, kernel_size, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
        "ax1.add_patch(rect)\n",
        "\n",
        "# Update function for animation\n",
        "def update(frame):\n",
        "    row = frame // (image.shape[1] - kernel_size)\n",
        "    col = frame % (image.shape[1] - kernel_size)\n",
        "\n",
        "    # Update rectangle position\n",
        "    rect.set_xy((col, row))\n",
        "\n",
        "    return img1, img2, rect\n",
        "\n",
        "# Number of steps in the convolution (valid mode: 4x4 output feature map)\n",
        "num_steps = feature_map.shape[0] * feature_map.shape[1]\n",
        "\n",
        "# Create animation\n",
        "ani = animation.FuncAnimation(fig, update, frames=num_steps, interval=500, blit=False)\n",
        "\n",
        "# Save animation\n",
        "animation_path = \"convolution_animation.gif\"\n",
        "ani.save(animation_path, writer=\"pillow\", fps=2)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "4LbANnutsCtn",
        "outputId": "8311e771-9143-4fd9-b602-7f6c2d40cd24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Model parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model_cnn = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Input(shape=input_shape),\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.GlobalAveragePooling2D(),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "qm74KU90MloI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Compile the CNN\n",
        "model_cnn.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "    ],\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "YzqdM8cMPjtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "\n",
        "model_cnn.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.15\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "yrYQI-7kPaI9",
        "outputId": "7afd1669-0d9f-4b69-9c41-4c79ef19798e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "predictions_cnn = model_cnn.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "GNTKhQGGPf5f",
        "outputId": "bdc3181b-a546-43a0-e0ed-302f6019af32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "precision_recall_fscore_support(y_test, predictions_cnn.argmax(axis=1))\n",
        "```"
      ],
      "metadata": {
        "id": "jPc2y-_EP9LZ",
        "outputId": "640e26c7-f8b6-4449-d839-7acafc852ea6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "hTFAUe_721c5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges and Factors in Neural Network Applications"
      ],
      "metadata": {
        "id": "ehibvL4VQo2s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IGvtAIojUFm"
      },
      "source": [
        "### Number of Hidden Layers\n",
        "\n",
        "- For many problems, you can begin with a single hidden layer and get reasonable results.\n",
        "- An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons.\n",
        "- For complex problems, deep networks have a much higher parameter efficiency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.\n",
        "\n",
        "To understand why, suppose you are asked to draw a forest using some drawing software, but you are forbidden to copy and paste anything. It would take an enormous amount of time: you would have to draw each tree individually, branch by branch, leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch, then copy and paste that branch to create a tree, and finally copy and paste this tree to make a forest, you would be finished in no time. Real-world data is often structured in such a hierarchical way, and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces).\n",
        "\n",
        "Not only does this hierarchical architecture help DNNs converge faster to a good solution, but it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called *transfer learning*.\n",
        "\n",
        "In summary, for many problems you can start with just one or two hidden layers and the neural network will work just fine. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones), and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8ZkVTfjUFm"
      },
      "source": [
        "### Number of Neurons per Hidden Layer\n",
        "\n",
        "- The number of neurons in the input and output layers is determined by the type of input and output your task requires. - For example, the MNIST task requires 28 × 28 = 784 input neurons and 10 output neurons.\n",
        "\n",
        "As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
        "\n",
        "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. But in practice, it’s often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting.\n",
        "\n",
        "Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants” approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid bottleneck layers that could ruin your model. On the flip side, if a layer has too few neurons, it will not have enough representational power to preserve all the useful information from the inputs (e.g., a layer with two neurons can only output 2D data, so if it processes 3D data, some information will be lost). No matter how big and powerful the rest of the network is, that information will never be recovered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eho-mmFQjUFn"
      },
      "source": [
        "### Learning Rate, Batch Size, and Other Hyperparameters\n",
        "\n",
        "- The numbers of hidden layers and neurons are not the only hyperparameters you can tweak in an MLP. Here are some of the most important ones, as well as tips on how to set them:\n",
        "\n",
        "\n",
        "* **Learning rate**    \n",
        "The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., $10^{-5}$) and gradually increasing it up to a very large value (e.g., $10$). This is done by multiplying the learning rate by a constant factor at each iteration to go from $10^{-5}$ to $10$ in $500$ iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about $10$ times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate.\n",
        "        \n",
        "            \n",
        "\n",
        "* **Optimizer**    \n",
        "Choosing a better optimizer than plain old Mini-batch Gradient Descent (and tuning its hyperparameters) is also quite important.\n",
        "\n",
        "    \n",
        "* **Batch size**    \n",
        "The batch size can have a significant impact on your model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently, so the training algorithm will see more instances per second. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. There’s a catch, though: in practice, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a smaller batch size. One strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead.\n",
        "    \n",
        "    \n",
        "* **Activation function**    \n",
        "In general, the `ReLU` activation function will be a good default for all hidden layers. For the output layer, it really depends on your task.\n",
        "\n",
        "    \n",
        "* **Number of iterations**    \n",
        "In most cases, the number of training iterations does not actually need to be tweaked: just use early stopping instead.\n",
        "\n",
        "For more best practices regarding tuning neural network hyperparameters, check out the excellent [2018 paper](https://arxiv.org/abs/1803.09820) by Leslie Smith."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Practice"
      ],
      "metadata": {
        "id": "JI-CCBZS24ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "8lb4LyuGSll8"
      }
    }
  ]
}