{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMxrp8MqI++yx09YrhMwSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/INFO371/blob/main/INFO371_Week9_NLP_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFnTQ33V0o3u",
        "outputId": "15e2326a-c0ea-41f7-97a7-1f12c9c1870c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual tensor: tensor([1, 2, 3])\n",
            "Random 2x3 tensor: tensor([[0.4764, 0.4802, 0.9512],\n",
            "        [0.6237, 0.9207, 0.2991]])\n",
            "Range tensor: tensor([0., 1., 2., 3., 4., 5.])\n",
            "Epoch 1, Batch 0, Loss: 1.4208\n",
            "Epoch 1, Batch 100, Loss: 1.3731\n",
            "Epoch 1, Batch 200, Loss: 1.3243\n",
            "Epoch 1, Batch 300, Loss: 1.2920\n",
            "Epoch 1 complete | Total Loss: 418.3657\n",
            "Epoch 2, Batch 0, Loss: 1.2665\n",
            "Epoch 2, Batch 100, Loss: 1.0192\n",
            "Epoch 2, Batch 200, Loss: 0.9617\n",
            "Epoch 2, Batch 300, Loss: 0.5262\n",
            "Epoch 2 complete | Total Loss: 275.5634\n",
            "Epoch 3, Batch 0, Loss: 0.6237\n",
            "Epoch 3, Batch 100, Loss: 0.5182\n",
            "Epoch 3, Batch 200, Loss: 0.5081\n",
            "Epoch 3, Batch 300, Loss: 0.5646\n",
            "Epoch 3 complete | Total Loss: 161.3407\n",
            "Model saved to agnews_nlp_model.pt\n",
            "Accuracy: 0.798\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.85      0.81       511\n",
            "           1       0.90      0.87      0.88       526\n",
            "           2       0.71      0.75      0.73       449\n",
            "           3       0.81      0.72      0.76       514\n",
            "\n",
            "    accuracy                           0.80      2000\n",
            "   macro avg       0.80      0.80      0.80      2000\n",
            "weighted avg       0.80      0.80      0.80      2000\n",
            "\n",
            "‚ùå Predicted 0, Actual 3: Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting agency went aft\n",
            "‚ùå Predicted 0, Actual 3: Loosing the War on Terrorism \\\\\"Sven Jaschan, self-confessed author of the Netsky and Sasser viruses\n",
            "‚ùå Predicted 2, Actual 3: E-mail scam targets police chief Wiltshire Police warns about \"phishing\" after its fraud squad chief\n",
            "‚ùå Predicted 0, Actual 3: Card fraud unit nets 36,000 cards In its first two years, the UK's dedicated card fraud unit, has re\n"
          ]
        }
      ],
      "source": [
        "# INFO 371: NLP Lab ‚Äì Full Lecture Parallel (AG News)\n",
        "# Author: Charles Dorner, EdD (Candidate)\n",
        "\n",
        "# ‚úÖ Fix AG_NEWS loading issue by installing torchdata\n",
        "!pip install torchdata --quiet\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# üîß Manual Tensor Examples (Matching Lecture Style)\n",
        "tensor_from_list = torch.tensor([1, 2, 3])\n",
        "print(\"Manual tensor:\", tensor_from_list)\n",
        "\n",
        "rand_tensor = torch.rand(2, 3)\n",
        "print(\"Random 2x3 tensor:\", rand_tensor)\n",
        "\n",
        "range_tensor = torch.arange(0, 6, dtype=torch.float32)\n",
        "print(\"Range tensor:\", range_tensor)\n",
        "\n",
        "# üì• Load AG News Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ AG News CSVs manually loaded (alternative to AG_NEWS())\n",
        "train_df = pd.read_csv(\"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\", header=None)\n",
        "test_df = pd.read_csv(\"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\", header=None)\n",
        "\n",
        "train_texts = train_df[1] + \" \" + train_df[2]\n",
        "train_labels = train_df[0] - 1  # Zero-index\n",
        "test_texts = test_df[1] + \" \" + test_df[2]\n",
        "test_labels = test_df[0] - 1\n",
        "\n",
        "\n",
        "# üî† Tokenization and Vocabulary\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=[\"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "# üß± Encoding and Padding\n",
        "def encode(text):\n",
        "    return torch.tensor(vocab(tokenizer(text)), dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list = [], []\n",
        "    for _text, _label in batch:\n",
        "        text_list.append(encode(_text))\n",
        "        label_list.append(torch.tensor(_label, dtype=torch.long))\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return text_list, torch.stack(label_list)\n",
        "\n",
        "# üì¶ Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_ds = TextDataset(train_texts[:10000], train_labels[:10000])\n",
        "test_ds = TextDataset(test_texts[:2000], test_labels[:2000])\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# üß† Model Definition\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        mean_emb = embedded.mean(dim=1)\n",
        "        x = self.relu(self.fc1(mean_emb))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ‚öôÔ∏è Training Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextClassifier(len(vocab), 64, 32, 4).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# üîÅ Training Loop\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dl):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch}, Loss: {loss.item():.4f}\")\n",
        "    print(f\"Epoch {epoch+1} complete | Total Loss: {total_loss:.4f}\")\n",
        "\n",
        "# üíæ Save Model\n",
        "torch.save(model.state_dict(), \"agnews_nlp_model.pt\")\n",
        "print(\"Model saved to agnews_nlp_model.pt\")\n",
        "\n",
        "# üì• Load Model\n",
        "model.load_state_dict(torch.load(\"agnews_nlp_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# üß™ Evaluation (NumPy-safe)\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dl:\n",
        "        X = X.to(device)\n",
        "        out = model(X)\n",
        "        preds = out.argmax(1).cpu().tolist()\n",
        "        y_true.extend(y.tolist())\n",
        "        y_pred.extend(preds)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "\n",
        "# üìä Optional: Misclassification Analysis\n",
        "for i in range(10):\n",
        "    if y_true[i] != y_pred[i]:\n",
        "        print(f\"‚ùå Predicted {y_pred[i]}, Actual {y_true[i]}: {test_texts[i][:100]}\")\n"
      ]
    }
  ]
}