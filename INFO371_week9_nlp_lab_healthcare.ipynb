{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqrcdYQBMlFQPmFec11UtR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/INFO371/blob/main/INFO371_week9_nlp_lab_healthcare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFnTQ33V0o3u",
        "outputId": "705a7818-4436-46b6-b711-1035a7a35807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual tensor: tensor([1, 2, 3])\n",
            "Random 2x3 tensor: tensor([[0.7209, 0.7654, 0.4252],\n",
            "        [0.8179, 0.0247, 0.5336]])\n",
            "Range tensor: tensor([0., 1., 2., 3., 4., 5.])\n",
            "Epoch 1, Batch 0, Loss: 0.7188\n",
            "Epoch 1, Batch 50, Loss: 0.4466\n",
            "Epoch 1, Batch 100, Loss: 0.0934\n",
            "Epoch 1 complete | Total Loss: 42.8276\n",
            "Epoch 2, Batch 0, Loss: 0.0457\n",
            "Epoch 2, Batch 50, Loss: 0.0133\n",
            "Epoch 2, Batch 100, Loss: 0.0056\n",
            "Epoch 2 complete | Total Loss: 1.6946\n",
            "Epoch 3, Batch 0, Loss: 0.0042\n",
            "Epoch 3, Batch 50, Loss: 0.0035\n",
            "Epoch 3, Batch 100, Loss: 0.0019\n",
            "Epoch 3 complete | Total Loss: 0.3514\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       false       1.00      1.00      1.00       512\n",
            "        true       1.00      1.00      1.00       488\n",
            "\n",
            "    accuracy                           1.00      1000\n",
            "   macro avg       1.00      1.00      1.00      1000\n",
            "weighted avg       1.00      1.00      1.00      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# INFO 371: NLP Lab ‚Äì Healthcare Dataset (Generated Manually)\n",
        "# Author: Charles Dorner, EdD (Candidate)\n",
        "\n",
        "!pip install torchdata --quiet\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import random\n",
        "\n",
        "# üîß Tensor Demos\n",
        "print(\"Manual tensor:\", torch.tensor([1, 2, 3]))\n",
        "print(\"Random 2x3 tensor:\", torch.rand(2, 3))\n",
        "print(\"Range tensor:\", torch.arange(0, 6, dtype=torch.float32))\n",
        "\n",
        "# üì• Generate healthcare-style dataset (truthful vs. misinformation)\n",
        "true_claims = [\n",
        "    \"Vaccines prevent serious illness and save lives.\",\n",
        "    \"Washing hands reduces the spread of infection.\",\n",
        "    \"Exercise improves cardiovascular health.\",\n",
        "    \"Wearing seat belts reduces fatalities in car accidents.\",\n",
        "    \"Early cancer screening increases survival rates.\",\n",
        "    \"Flu shots are recommended annually for most people.\",\n",
        "    \"Smoking increases the risk of lung cancer.\",\n",
        "    \"Obesity is a risk factor for type 2 diabetes.\",\n",
        "    \"Proper hydration supports kidney function.\",\n",
        "    \"Sleep deprivation affects immune response.\"\n",
        "]\n",
        "\n",
        "false_claims = [\n",
        "    \"Vaccines cause autism in children.\",\n",
        "    \"Drinking bleach cures viral infections.\",\n",
        "    \"You can detox your body with foot pads.\",\n",
        "    \"Microwaving food removes its nutrients.\",\n",
        "    \"Essential oils cure cancer.\",\n",
        "    \"5G towers cause COVID-19.\",\n",
        "    \"Wearing wet socks cures the flu.\",\n",
        "    \"Alkaline water neutralizes all disease.\",\n",
        "    \"Sunlight alone can replace a healthy diet.\",\n",
        "    \"Ear candles remove brain toxins.\"\n",
        "]\n",
        "\n",
        "# Build dataset of 5000 entries (balanced)\n",
        "reviews, sentiments = [], []\n",
        "for _ in range(2500):\n",
        "    reviews.append(random.choice(true_claims))\n",
        "    sentiments.append(1)  # True\n",
        "\n",
        "    reviews.append(random.choice(false_claims))\n",
        "    sentiments.append(0)  # False\n",
        "\n",
        "# Shuffle\n",
        "combined = list(zip(reviews, sentiments))\n",
        "random.shuffle(combined)\n",
        "reviews, sentiments = zip(*combined)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\"review\": reviews, \"sentiment\": sentiments})\n",
        "\n",
        "# Split manually\n",
        "train_texts = df[\"review\"][:4000]\n",
        "train_labels = df[\"sentiment\"][:4000]\n",
        "test_texts = df[\"review\"][4000:]\n",
        "test_labels = df[\"sentiment\"][4000:]\n",
        "label_names = [\"false\", \"true\"]\n",
        "\n",
        "# üî† Tokenizer and Vocabulary\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=[\"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "def encode(text):\n",
        "    return torch.tensor(vocab(tokenizer(text)), dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list = [], []\n",
        "    for _text, _label in batch:\n",
        "        text_list.append(encode(_text))\n",
        "        label_list.append(torch.tensor(_label, dtype=torch.long))\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return text_list, torch.stack(label_list)\n",
        "\n",
        "# üì¶ Dataset and DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts.iloc[idx], self.labels.iloc[idx]\n",
        "\n",
        "train_ds = TextDataset(train_texts, train_labels)\n",
        "test_ds = TextDataset(test_texts, test_labels)\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# üß† Model Definition\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        mean_emb = embedded.mean(dim=1)\n",
        "        x = self.relu(self.fc1(mean_emb))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ‚öôÔ∏è Training Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextClassifier(len(vocab), 64, 32, 2).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# üîÅ Training Loop\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dl):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if batch % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch}, Loss: {loss.item():.4f}\")\n",
        "    print(f\"Epoch {epoch+1} complete | Total Loss: {total_loss:.4f}\")\n",
        "\n",
        "# üíæ Save and Reload Model\n",
        "torch.save(model.state_dict(), \"healthcare_sentiment_model.pt\")\n",
        "model.load_state_dict(torch.load(\"healthcare_sentiment_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# üß™ Evaluation\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dl:\n",
        "        X = X.to(device)\n",
        "        out = model(X)\n",
        "        preds = out.argmax(1).cpu().tolist()\n",
        "        y_true.extend(y.tolist())\n",
        "        y_pred.extend(preds)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=label_names))\n",
        "\n",
        "# üîç Misclassifications\n",
        "for i in range(10):\n",
        "    if y_true[i] != y_pred[i]:\n",
        "        print(f\"‚ùå Predicted {label_names[y_pred[i]]}, Actual {label_names[y_true[i]]}: {test_texts.iloc[i][:100]}\")\n"
      ]
    }
  ]
}