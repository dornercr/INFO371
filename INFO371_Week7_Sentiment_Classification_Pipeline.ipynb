{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPejD0a95RqxRgl0sdXbLLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/INFO371/blob/main/INFO371_Week7_Sentiment_Classification_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxvipswZ0bk3",
        "outputId": "becbe65e-2357-4d29-bd80-190790e480e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 108.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” BoW Results:\n",
            "CV Accuracy: 0.56\n",
            "Test Accuracy: 0.33\n",
            "Precision: 0.33\n",
            "Recall: 1.0\n",
            "F1 Score: 0.5\n",
            "\n",
            "ğŸ” TF-IDF Results:\n",
            "CV Accuracy: 0.56\n",
            "Test Accuracy: 0.33\n",
            "Precision: 0.33\n",
            "Recall: 1.0\n",
            "F1 Score: 0.5\n",
            "\n",
            "ğŸ” Word Embeddings Results:\n",
            "CV Accuracy: 0.56\n",
            "Test Accuracy: 0.67\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ğŸ§  Step 2: Load spaCy model for tokenization + embeddings\n",
        "!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load(\"en_core_web_md\")  # This includes 300-dim word vectors\n",
        "\n",
        "# ğŸ§¹ Custom spaCy tokenizer used for BoW and TF-IDF\n",
        "def spacy_tokenizer(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_.lower().strip() for token in doc\n",
        "            if not token.is_stop and not token.is_punct and token.lemma_.strip()]\n",
        "\n",
        "# ğŸ§ª Step 3: Define a simple toy sentiment dataset\n",
        "toy_messages = [\n",
        "    \"I absolutely loved this product!\",\n",
        "    \"This is the worst thing I've ever used.\",\n",
        "    \"Fantastic experience overall.\",\n",
        "    \"Terrible customer service.\",\n",
        "    \"Will definitely buy again.\",\n",
        "    \"Horrible. Just horrible.\",\n",
        "    \"Five stars, no complaints!\",\n",
        "    \"Disappointed and frustrated.\",\n",
        "    \"Everything went smoothly and I'm happy.\",\n",
        "    \"It broke on day one.\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "\n",
        "df = pd.DataFrame({\"message\": toy_messages, \"label\": labels})\n",
        "\n",
        "# ğŸ§® Step 4A: Bag-of-Words Vectorization\n",
        "bow_vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n",
        "X_bow = bow_vectorizer.fit_transform(df[\"message\"])\n",
        "\n",
        "# ğŸ§® Step 4B: TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[\"message\"])\n",
        "\n",
        "# ğŸ§  Step 4C: Word Embeddings (Mean of word vectors)\n",
        "def get_embedding(text):\n",
        "    return nlp(text).vector  # average of all word vectors\n",
        "\n",
        "X_embed = np.array([get_embedding(msg) for msg in tqdm(df[\"message\"])])\n",
        "\n",
        "# ğŸ§ª Step 5: Model Training and Evaluation Function\n",
        "def evaluate_knn(X, y, k=3):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    cv_score = cross_val_score(clf, X_train, y_train, cv=3).mean()\n",
        "    clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        \"CV Accuracy\": round(cv_score, 2),\n",
        "        \"Test Accuracy\": round(accuracy_score(y_test, preds), 2),\n",
        "        \"Precision\": round(precision_score(y_test, preds), 2),\n",
        "        \"Recall\": round(recall_score(y_test, preds), 2),\n",
        "        \"F1 Score\": round(f1_score(y_test, preds), 2)\n",
        "    }\n",
        "\n",
        "# ğŸ“Š Step 6: Run evaluations for each feature set\n",
        "results = {\n",
        "    \"BoW\": evaluate_knn(X_bow, df[\"label\"]),\n",
        "    \"TF-IDF\": evaluate_knn(X_tfidf, df[\"label\"]),\n",
        "    \"Word Embeddings\": evaluate_knn(X_embed, df[\"label\"])\n",
        "}\n",
        "\n",
        "# ğŸ–¨ï¸ Show results\n",
        "for model_type, metrics in results.items():\n",
        "    print(f\"\\nğŸ” {model_type} Results:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n"
      ]
    }
  ]
}