{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrxNdN41/l2splEjyxjIfu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/INFO371/blob/main/week3_Customer_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5lOJngHDfQ8"
      },
      "outputs": [],
      "source": [
        "ğŸ”¸ Section: Load the Iris Flower Dataset\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "Weâ€™re loading the Iris dataset directly from GitHub using a URL. Pandas reads it and stores it in a DataFrame named df.\n",
        "\n",
        "df.shape\n",
        "\n",
        "This line returns the shape of the dataset â€” in this case, 150 rows and 5 columns. That means we have 150 samples, and each sample has four measurements plus one label column for the species.\n",
        "ğŸ”¸ Section: Explore Target Classes\n",
        "\n",
        "species = df.species.unique()\n",
        "species\n",
        "\n",
        "This code returns the distinct values in the species column. We should see three species: Setosa, Versicolor, and Virginica. These are our three target classes.\n",
        "ğŸ”¸ Section: Convert Labels to Numeric\n",
        "\n",
        "label_maps = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
        "y_train = df.species.map(label_maps)\n",
        "\n",
        "Machine learning models require numeric labels, so we map each species to an integer: Setosa is 0, Versicolor is 1, and Virginica is 2. The .map() function applies this transformation to every row in the species column, producing our label vector y_train.\n",
        "ğŸ”¸ Section: Extract Features\n",
        "\n",
        "X_train = df.iloc[:, 0:-1]\n",
        "\n",
        "This selects all rows and the first four columns â€” the feature columns â€” excluding the last column, which contains the species labels. The result is X_train, a matrix of shape 150 by 4.\n",
        "\n",
        "X_train.head()\n",
        "\n",
        "Displays the first five rows of the feature matrix, giving us a quick look at the raw numeric data used to train our model.\n",
        "ğŸ”¸ Section: Train K-Nearest Neighbors Classifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "We import KNN from scikit-learn, set the number of neighbors to 5, and fit the model using the training data. KNN is a lazy learner â€” it doesnâ€™t actually do much during training except store the training data. The real work happens during prediction.\n",
        "\n",
        "knn.score(X_train, y_train)\n",
        "\n",
        "This returns the model's accuracy on the training set. With this small and clean dataset, youâ€™ll see an accuracy around 96 or 97 percent.\n",
        "ğŸ”¸ Section: Predict on Training Data\n",
        "\n",
        "knn.predict(X_train)\n",
        "\n",
        "This returns the predicted class for each of the 150 samples using the trained KNN model.\n",
        "ğŸ”¸ Section: Evaluate Accuracy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_train, knn.predict(X_train))\n",
        "\n",
        "We import accuracy_score to compute how many predictions matched the actual labels. Again, this is on the training set, so itâ€™s expected to be high. But it's not a good measure of generalization.\n",
        "ğŸ”¸ Section: Evaluate with Precision, Recall, F1 Score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "y_pred = knn.predict(X_train)\n",
        "precision_recall_fscore_support(y_train, y_pred)\n",
        "\n",
        "This function gives us:\n",
        "\n",
        "    Precision: the percentage of predicted labels that were correct\n",
        "\n",
        "    Recall: the percentage of actual labels we correctly identified\n",
        "\n",
        "    F1 Score: the harmonic mean of precision and recall\n",
        "\n",
        "    Support: the number of true instances for each class\n",
        "\n",
        "These are provided for each of the three species.\n",
        "ğŸ”¸ Section: Motivation for Rescaling\n",
        "\n",
        "    Text: â€œMany techniques are sensitive to the scale of your dataâ€¦â€\n",
        "\n",
        "The next block introduces a different dataset â€” heights and weights of people â€” to show why feature scaling matters.\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Person\": ['A', 'B', 'C'],\n",
        "    \"height (cm)\": [160, 170.2, 177.8],\n",
        "    \"weight (pounds)\": [150, 160, 171],\n",
        "    \"height (inches)\": [63, 67, 70]\n",
        "})\n",
        "\n",
        "We create a toy dataset with three people. The same concept â€” height â€” appears in different units (centimeters and inches), along with weight.\n",
        "ğŸ”¸ Section: Distance Without Scaling\n",
        "\n",
        "from scipy.spatial import distance\n",
        "print(\"A to B:\", distance.euclidean(df.iloc[0, 2:], df.iloc[1, 2:]))\n",
        "print(\"A to C:\", distance.euclidean(df.iloc[0, 2:], df.iloc[2, 2:]))\n",
        "print(\"B to C:\", distance.euclidean(df.iloc[1, 2:], df.iloc[2, 2:]))\n",
        "\n",
        "We calculate Euclidean distances between pairs of people using raw values from the DataFrame. The distances vary widely, and results differ depending on which units we choose. Thatâ€™s a red flag.\n",
        "ğŸ”¸ Section: Manual Z-Score Scaling\n",
        "\n",
        "df_data = df.set_index('Person')\n",
        "df_scaled = (df_data - df_data.mean(axis=0)) / df_data.std(axis=0)\n",
        "\n",
        "We set â€œPersonâ€ as the index and scale the numeric columns using z-score normalization â€” subtracting the mean and dividing by the standard deviation. Now, all features have mean 0 and standard deviation 1.\n",
        "ğŸ”¸ Section: Verify Scaled Features\n",
        "\n",
        "df_scaled.mean(axis = 0)\n",
        "df_scaled.std(axis = 0)\n",
        "\n",
        "These show the new means and standard deviations, confirming that scaling worked as expected.\n",
        "ğŸ”¸ Section: Distance After Scaling\n",
        "\n",
        "distance.euclidean(df_scaled.iloc[0], df_scaled.iloc[1])\n",
        "\n",
        "We recalculate the distances between individuals after scaling. This time, the differences are more balanced and independent of units.\n",
        "ğŸ”¸ Section: Scale with StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_data)\n",
        "scaled = scaler.transform(df_data)\n",
        "\n",
        "Here, we use scikit-learn's built-in scaler. We first .fit() the scaler to compute the mean and std, then .transform() to apply the scaling.\n",
        "ğŸ”¸ Section: Validate Scaled Output\n",
        "\n",
        "scaled.mean(axis = 0)\n",
        "scaled.std(axis = 0)\n",
        "\n",
        "We verify that the transformed data has a mean of 0 and standard deviation of 1 â€” consistent with our manual approach.\n",
        "ğŸ”¸ Section: Population vs Sample Variance\n",
        "\n",
        "df_data.var(axis = 0)\n",
        "scaler.var_\n",
        "\n",
        "df_data.var() uses nâˆ’1nâˆ’1, which is sample variance.\n",
        "scaler.var_ uses nn, which is population variance.\n",
        "The values differ slightly â€” good to understand, but not usually critical for ML models.\n",
        "ğŸ”¸ Section: Curse of Dimensionality\n",
        "\n",
        "    Text: â€œk-nearest neighbors runs into trouble in higher dimensionsâ€¦â€\n",
        "\n",
        "As dimensions increase, data becomes sparse, and distances become less meaningful. This is the Curse of Dimensionality.\n",
        "ğŸ”¸ Section: Simulate Distance in High Dimensions\n",
        "\n",
        "def random_distances(dim, num_pairs):\n",
        "    return [distance.euclidean(np.random.rand(dim), np.random.rand(dim)) for _ in range(num_pairs)]\n",
        "\n",
        "We define a function to generate num_pairs random points in dim dimensions and compute their Euclidean distances.\n",
        "ğŸ”¸ Section: Run the Simulation\n",
        "\n",
        "dimensions = range(1, 101, 5)\n",
        "avg_distances = []\n",
        "min_distances = []\n",
        "\n",
        "for dim in dimensions:\n",
        "    distances = random_distances(dim, 10000)\n",
        "    avg_distances.append(np.mean(distances))\n",
        "    min_distances.append(min(distances))\n",
        "\n",
        "For each dimension from 1 to 100, we generate 10,000 distances and compute the average and minimum for that dimension.\n",
        "ğŸ”¸ Section: Visualize Distance Ratio\n",
        "\n",
        "plt.plot(list(dimensions), np.array(min_distances) / np.array(avg_distances))\n",
        "\n",
        "This plot shows how the ratio of minimum to average distance approaches 1 as dimension increases.\n",
        "Meaning: in high dimensions, everything is far away â€” and the nearest neighbor is almost as far as a random point.\n",
        "ğŸ”¸ Section: Visualizing Sparsity\n",
        "\n",
        "    Page 12â€“13: Random points in 1D, 2D, and 3D\n",
        "\n",
        "We see how 50 points cover the space well in 1D, somewhat in 2D, and poorly in 3D. As dimensionality increases, we need exponentially more data to achieve the same coverage.\n",
        "ğŸ”¸ Section: KNN and Dimensionality Reduction\n",
        "\n",
        "    Page 14â€“15\n",
        "\n",
        "KNN performance drops in high dimensions. We often use dimensionality reduction techniques like PCA to project data into a lower-dimensional subspace before applying KNN.\n",
        "ğŸ”¸ Section: KNN in scikit-learn\n",
        "\n",
        "    KNeighborsClassifier: standard k-NN classifier\n",
        "\n",
        "    RadiusNeighborsClassifier: finds all neighbors within a fixed radius\n",
        "\n",
        "    weights='uniform' uses majority vote\n",
        "\n",
        "    weights='distance' gives more weight to closer neighbors\n",
        "\n",
        "Choose k carefully: too small leads to noise sensitivity, too large oversmooths class boundaries."
      ]
    }
  ]
}